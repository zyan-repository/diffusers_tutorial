{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce979b6b",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第五节 Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot as plt\n",
    "# 这次要探索的管线比较多\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline, \n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionInpaintPipeline, \n",
    "    StableDiffusionDepth2ImgPipeline\n",
    ")\n",
    "# 因为要用到的展示图片较多，所以我们写了一个旨在下载图片的函数\n",
    "@retry_on_exception\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "# Inpainting需要用到的图片\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = download_image(img_url).resize((512, 512))\n",
    "mask_image = download_image(mask_url).resize((512, 512))\n",
    "\n",
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02329e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入管线\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "cache_dir = r'E:\\model_zoo\\huggingface\\diffusers'\n",
    "# 如果显存不足请指定revision=\"fp16\",torch_dtype=torch.float16\n",
    "@retry_on_exception\n",
    "def load_stable_diffusion_pipeline(model_id, cache_dir, torch_dtype=torch.float32, device='cuda'):\n",
    "    return StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch_dtype, cache_dir=cache_dir).to(device)\n",
    "\n",
    "pipe = load_stable_diffusion_pipeline(model_id, cache_dir, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力切分功能，降低速度减少GPU占用\n",
    "# pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af692584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给生成器设置一个随机种子，这样可以保证结果的可复现性\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "# 运行这个管线\n",
    "pipe_output = pipe(\n",
    "    prompt=\"Palette knife painting of an autumn cityscape\",\n",
    "    # 提示文字：哪些要生成\n",
    "    negative_prompt=\"Oversaturated, blurry, low quality\",\n",
    "    # 提示文字：哪些不要生成\n",
    "    height=640, width=640,     # 定义所生成图片的尺寸\n",
    "    guidance_scale=8,          # 提示文字的影响程度\n",
    "    num_inference_steps=50,    # 定义一次生成需要多少个推理步骤\n",
    "    generator=generator        # 设定随机种子的生成器\n",
    ")\n",
    "# 查看生成结果\n",
    "pipe_output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要的调节参数如下：\n",
    "# ● width和height用于指定所生成图片的尺寸，注意它们必须是能被8整除的数字，因为只有这样，VAE才能正常工作（原因稍后介绍）。\n",
    "# ● 步数num_inference_steps也会影响所生成图片的质量，采用默认设置50即可，你也可以尝试将其设置为20并观察效果。\n",
    "# ● negative_prompt用于强调不希望生成的内容，这个参数一般在无分类器引导的情况下使用。这种添加额外控制的方式特别有效：\n",
    "#    列出一些不想要的特征，以帮助生成更好的结果。\n",
    "# ● guidance_scale 决定了无分类器引导的影响强度。增大这个参数可以使生成的内容更接近给出的文本提示语；\n",
    "#    但如果该参数过大，则可能导致结果过于饱和，不美观。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加大guidance_scale参数的作用\n",
    "cfg_scales = [1.1, 8, 12] \n",
    "prompt = \"A collie with a pink hat\" \n",
    "fig, axs = plt.subplots(1, len(cfg_scales), figsize=(16, 5))\n",
    "for i, ax in enumerate(axs):\n",
    "    im = pipe(prompt, height=480, width=480, \n",
    "              guidance_scale=cfg_scales[i], num_inference_steps=35, \n",
    "              generator=torch.Generator(device=device).manual_seed(42)).images[0] \n",
    "    ax.imshow(im); ax.set_title(f'CFG Scale {cfg_scales[i]}')\n",
    "    \n",
    "# 一般来说 guidance_scale 在 8-12 是不错的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19461d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(pipe.components.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE（可变分自编码器）\n",
    "\n",
    "# 创建取值区间为(-1, 1)的伪数据\n",
    "images = torch.rand(1, 3, 512, 512).to(device) * 2 - 1 \n",
    "print(\"Input images shape:\", images.shape)\n",
    "# 编码到隐空间\n",
    "with torch.no_grad():\n",
    "    latents = 0.18215 * pipe.vae.encode(images).latent_dist.mean\n",
    "print(\"Encoded latents shape:\", latents.shape)\n",
    "# 再解码回来\n",
    "with torch.no_grad():\n",
    "    decoded_images = pipe.vae.decode(latents / 0.18215).sample\n",
    "print(\"Decoded images shape:\", decoded_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8329cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词器和文本编码器\n",
    "\n",
    "# 手动对提示文字进行分词和编码\n",
    "# 分词\n",
    "input_ids = pipe.tokenizer([\"A painting of a flooble\"])['input_ids']\n",
    "print(\"Input ID -> decoded token\")\n",
    "for input_id in input_ids[0]:\n",
    "    print(f\"{input_id} -> {pipe.tokenizer.decode(input_id)}\")\n",
    "# 将分词结果输入CLIP \n",
    "input_ids = torch.tensor(input_ids).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = pipe.text_encoder(input_ids)['last_hidden_state']\n",
    "    print(\"Text embeddings shape:\", text_embeddings.shape)\n",
    "\n",
    "# 输出最终编码结果\n",
    "text_embeddings = pipe._encode_prompt(\"A painting of a flooble\", device, 1, False, '')\n",
    "print(text_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet\n",
    "\n",
    "# 创建伪输入\n",
    "timestep = pipe.scheduler.timesteps[0]\n",
    "latents = torch.randn(1, 4, 64, 64).to(device)\n",
    "text_embeddings = torch.randn(1, 77, 1024).to(device)\n",
    "# 让模型进行预测\n",
    "with torch.no_grad():\n",
    "    unet_output = pipe.unet(latents, timestep, text_embeddings).sample\n",
    "print('UNet output shape:', unet_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14786c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调度器\n",
    "# 调度器保存了关于如何添加噪声的信息，并管理如何基于模型的预测更新“带噪”样本。\n",
    "# 默认调度器是 PNDMScheduler，也可以使用其他调度器如 LMSDiscreteScheduler\n",
    "\n",
    "# 观察采样过程中噪声的变化\n",
    "plt.plot(pipe.scheduler.alphas_cumprod, label=r'$\\bar{\\alpha}$')\n",
    "plt.xlabel('Timestep (high noise to low noise ->)')\n",
    "plt.title('Noise schedule')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "# 替换原来的调度器\n",
    "pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "# 输出配置参数\n",
    "print('Scheduler config:', pipe.scheduler)\n",
    "# 使用新的调度器生成图片\n",
    "pipe(prompt=\"Palette knife painting of an winter cityscape\", \n",
    "     height=480, width=480, generator=torch.Generator(device=device).manual_seed(42)).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 8\n",
    "num_inference_steps=30\n",
    "prompt = \"Beautiful picture of a wave breaking\"\n",
    "negative_prompt = \"zoomed in, blurry, oversaturated, warped\"\n",
    "\n",
    "# 对提示文字进行编码\n",
    "text_embeddings = pipe._encode_prompt(prompt, device, 1, True, negative_prompt)\n",
    "# 创建随机噪声作为起点\n",
    "latents = torch.randn((1, 4, 64, 64), device=device, generator=generator)\n",
    "latents *= pipe.scheduler.init_noise_sigma\n",
    "# 准备调度器\n",
    "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "# 生成过程开始\n",
    "for i, t in enumerate(pipe.scheduler.timesteps):\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "# 将隐变量映射到图片\n",
    "with torch.no_grad():\n",
    "    image = pipe.decode_latents(latents.detach())\n",
    "\n",
    "pipe.numpy_to_pil(image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他pipeline介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e710637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d54e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
